{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b23c29d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Store_id</th>\n",
       "      <th>Store_Type</th>\n",
       "      <th>Location_Type</th>\n",
       "      <th>Region_Code</th>\n",
       "      <th>Date</th>\n",
       "      <th>Holiday</th>\n",
       "      <th>Discount</th>\n",
       "      <th>#Order</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T1000001</td>\n",
       "      <td>1</td>\n",
       "      <td>S1</td>\n",
       "      <td>L3</td>\n",
       "      <td>R1</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>9</td>\n",
       "      <td>7011.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T1000002</td>\n",
       "      <td>253</td>\n",
       "      <td>S4</td>\n",
       "      <td>L2</td>\n",
       "      <td>R1</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>60</td>\n",
       "      <td>51789.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T1000003</td>\n",
       "      <td>252</td>\n",
       "      <td>S3</td>\n",
       "      <td>L2</td>\n",
       "      <td>R1</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>42</td>\n",
       "      <td>36868.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T1000004</td>\n",
       "      <td>251</td>\n",
       "      <td>S2</td>\n",
       "      <td>L3</td>\n",
       "      <td>R1</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>23</td>\n",
       "      <td>19715.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T1000005</td>\n",
       "      <td>250</td>\n",
       "      <td>S2</td>\n",
       "      <td>L3</td>\n",
       "      <td>R4</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>62</td>\n",
       "      <td>45614.52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  Store_id Store_Type Location_Type Region_Code        Date  \\\n",
       "0  T1000001         1         S1            L3          R1  2018-01-01   \n",
       "1  T1000002       253         S4            L2          R1  2018-01-01   \n",
       "2  T1000003       252         S3            L2          R1  2018-01-01   \n",
       "3  T1000004       251         S2            L3          R1  2018-01-01   \n",
       "4  T1000005       250         S2            L3          R4  2018-01-01   \n",
       "\n",
       "   Holiday Discount  #Order     Sales  \n",
       "0        1      Yes       9   7011.84  \n",
       "1        1      Yes      60  51789.12  \n",
       "2        1      Yes      42  36868.20  \n",
       "3        1      Yes      23  19715.16  \n",
       "4        1      Yes      62  45614.52  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file = r'C:\\Users\\girip\\Downloads\\TRAIN.csv'   # <- put your real path here\n",
    "dataset = pd.read_csv(file)\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff35550a",
   "metadata": {},
   "source": [
    "##Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "54963bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure Date is datetime, sort by Store_id & Date, then create features and rolling mean\n",
    "dataset['Date'] = pd.to_datetime(dataset['Date'])\n",
    "dataset = dataset.sort_values(['Store_id', 'Date']).reset_index(drop=True)\n",
    "\n",
    "dataset['Year'] = dataset['Date'].dt.year\n",
    "dataset['Month'] = dataset['Date'].dt.month\n",
    "dataset['DayOfWeek'] = dataset['Date'].dt.dayofweek\n",
    "\n",
    "dataset['Sales_Rolling_Mean_7D'] = dataset.groupby('Store_id')['Sales'] \\\n",
    "    .transform(lambda x: x.rolling(window=7, min_periods=1).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc836af",
   "metadata": {},
   "source": [
    "#Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c6a18193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical features scaled successfully.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Use numeric columns that the API expects (and that will be model inputs)\n",
    "numerical_features = ['Store_id', 'Year', 'Month', 'DayOfWeek']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "dataset[numerical_features] = scaler.fit_transform(dataset[numerical_features])\n",
    "\n",
    "print(\"Numerical features scaled successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4ef15753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features one-hot encoded successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Identify categorical features to be one-hot encoded\n",
    "categorical_features = ['Store_Type', 'Location_Type', 'Region_Code', 'Discount']\n",
    "\n",
    "# Apply one-hot encoding (do not drop first to keep consistent column set with the Flask API)\n",
    "dataset = pd.get_dummies(dataset, columns=categorical_features, drop_first=False)\n",
    "\n",
    "print(\"Categorical features one-hot encoded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9a1ca2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = dataset.drop(['Sales', 'ID', 'Date'], axis=1)\n",
    "y_train = dataset['Sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7db06a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression model re-trained on the entire X_train and y_train datasets.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "final_model_lr = LinearRegression()\n",
    "final_model_lr.fit(X_train, y_train)\n",
    "\n",
    "print(\"Linear Regression model re-trained on the entire X_train and y_train datasets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1d4388e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression model saved to 'linear_regression_model.joblib'.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained Linear Regression model to a file\n",
    "joblib.dump(final_model_lr, 'linear_regression_model.joblib')\n",
    "\n",
    "print(\"Linear Regression model saved to 'linear_regression_model.joblib'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a22fbd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler saved to 'scaler.joblib'.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the fitted StandardScaler instance to a file\n",
    "joblib.dump(scaler, 'scaler.joblib')\n",
    "print(\"StandardScaler saved to 'scaler.joblib'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "942e754c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['encoded_columns.joblib']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(final_model_lr, 'linear_regression_model.joblib')\n",
    "joblib.dump(scaler, 'scaler.joblib')\n",
    "# Save the final X_train columns in order\n",
    "joblib.dump(X_train.columns.tolist(), 'X_train_columns.joblib')\n",
    "# And save encoded column list if your Flask loads it:\n",
    "encoded_columns = [c for c in X_train.columns if c not in ['Store_id','Year','Month','DayOfWeek','Holiday']]\n",
    "joblib.dump(encoded_columns, 'encoded_columns.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6dc7717b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daily_sales DataFrame re-created and indexed by Date successfully.\n",
      "Training data contains data up to 2019-02-16\n",
      "Validation data contains data from 2019-02-17 to 2019-05-31\n",
      "Shape of training data: (412, 1)\n",
      "Shape of validation data: (104, 1)\n",
      "Head of re-indexed daily_sales:\n",
      "                 Sales\n",
      "Date                  \n",
      "2018-01-01  15345484.5\n",
      "2018-01-02  19592415.0\n",
      "2018-01-03  18652527.0\n",
      "2018-01-04  19956267.0\n",
      "2018-01-05  22902651.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Group the original dataset by the 'Date' column and sum the 'Sales' to re-create daily_sales\n",
    "daily_sales = dataset.groupby('Date')['Sales'].sum().reset_index()\n",
    "\n",
    "# 2. Convert the 'Date' column to datetime objects and then set it as the DataFrame's index\n",
    "daily_sales['Date'] = pd.to_datetime(daily_sales['Date'])\n",
    "daily_sales.set_index('Date', inplace=True)\n",
    "\n",
    "# 3. Define the split_date\n",
    "split_date = '2019-02-16'\n",
    "\n",
    "# 4. Filter daily_sales to create train_data\n",
    "train_data = daily_sales[daily_sales.index <= split_date]\n",
    "\n",
    "# 5. Filter daily_sales to create validation_data\n",
    "validation_data = daily_sales[daily_sales.index > split_date]\n",
    "\n",
    "print(\"daily_sales DataFrame re-created and indexed by Date successfully.\")\n",
    "print(f\"Training data contains data up to {train_data.index.max().strftime('%Y-%m-%d')}\")\n",
    "print(f\"Validation data contains data from {validation_data.index.min().strftime('%Y-%m-%d')} to {validation_data.index.max().strftime('%Y-%m-%d')}\")\n",
    "print(f\"Shape of training data: {train_data.shape}\")\n",
    "print(f\"Shape of validation data: {validation_data.shape}\")\n",
    "print(\"Head of re-indexed daily_sales:\")\n",
    "print(daily_sales.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fa19f4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\girip\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\girip\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\girip\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARIMA model fitted successfully.\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "arima_order = (5, 1, 0)\n",
    "\n",
    "\n",
    "model_arima = ARIMA(train_data['Sales'], order=arima_order)\n",
    "model_arima_fit = model_arima.fit()\n",
    "\n",
    "print(\"ARIMA model fitted successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bdaa9b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARIMA model saved to 'arima_model.joblib'.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained ARIMA model to a file\n",
    "joblib.dump(model_arima_fit, 'arima_model.joblib')\n",
    "\n",
    "print(\"ARIMA model saved to 'arima_model.joblib'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "146232e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost not available, falling back to sklearn HistGradientBoostingRegressor: No module named 'xgboost'\n",
      "Trained HistGradientBoostingRegressor (sklearn fallback) on inference-ready features.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "# Train XGBoost if available, otherwise fall back to sklearn's HistGradientBoostingRegressor\n",
    "# Use only features available at inference time (exclude engineered/time-series features)\n",
    "# such as '#Order' and 'Sales_Rolling_Mean_7D' which cannot be computed from a single request.\n",
    "xgb_features = [c for c in X_train.columns if c not in ['#Order', 'Sales_Rolling_Mean_7D']]\n",
    "X_train_xgb = X_train[xgb_features]\n",
    "try:\n",
    "    from xgboost import XGBRegressor  # may raise ImportError\n",
    "    model_xgb_final = XGBRegressor(random_state=42)\n",
    "    model_xgb_final.fit(X_train_xgb, y_train)\n",
    "    print(\"Trained XGBRegressor (xgboost) on inference-ready features.\")\n",
    "except Exception as e:\n",
    "    print(\"xgboost not available, falling back to sklearn HistGradientBoostingRegressor:\", e)\n",
    "    model_xgb_final = HistGradientBoostingRegressor(random_state=42)\n",
    "    model_xgb_final.fit(X_train_xgb, y_train)\n",
    "    print(\"Trained HistGradientBoostingRegressor (sklearn fallback) on inference-ready features.\")\n",
    "\n",
    "# Ensure scaler_current exists for downstream saving\n",
    "if 'scaler_current' not in globals():\n",
    "    scaler_current = scaler\n",
    "    print(\"Set scaler_current = scaler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "14937fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoostRegressor model saved to model_xgb_final.joblib\n",
      "Scaler saved to scaler_current.joblib\n",
      "XGBoost feature column names saved to X_train_columns.joblib\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Define paths for saving artifacts\n",
    "model_path = 'model_xgb_final.joblib'\n",
    "scaler_path = 'scaler_current.joblib'\n",
    "columns_path = 'X_train_columns.joblib'\n",
    "\n",
    "# Save the trained XGBoostRegressor model\n",
    "joblib.dump(model_xgb_final, model_path)\n",
    "print(f\"XGBoostRegressor model saved to {model_path}\")\n",
    "\n",
    "# Save the scaler used for numerical features\n",
    "joblib.dump(scaler_current, scaler_path)\n",
    "print(f\"Scaler saved to {scaler_path}\")\n",
    "\n",
    "# Save the column names used for XGBoost (the inference-ready feature order)\n",
    "# If xgb_features was defined during training, save that order; otherwise save X_train.columns\n",
    "try:\n",
    "    cols_to_save = xgb_features\n",
    "except NameError:\n",
    "    cols_to_save = X_train.columns.tolist()\n",
    "joblib.dump(cols_to_save, columns_path)\n",
    "print(f\"XGBoost feature column names saved to {columns_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
